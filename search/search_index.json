{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Structured Streamer \u00b6 struct_strm (structured streamer) is a Python package that makes it easy to stream partial json generated by LLMs into valid json responses. This enables partial rendering of UI components without needing to wait for a full response, drastically reducing the time to the first word on the user's screen. Why Use Structured Streamer? \u00b6 JSON format is the standard when dealing with structured responses from LLMs. In the early days of LLM structured generation we had to validate the JSON response only after the whole JSON response had been returned. Modern approaches use constrained decoding to ensure that only valid json is returned, eliminating the need for post generation validation, and allowing us to use the response imediately. However, the streamed json response is incomplete, so it can't be parsed using traditional methods. This library aims to make it easier to handle this partially generated json to provide a better end user experience. See the benchmarks section in the docs for more details about how this library can speed up your structured response processing. You can learn more about constrained decoding and context free grammar here: XGrammar - Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar Installation \u00b6 1 pip install struct-strm Main Features \u00b6 The primary feature is to wrap LLM outputs to produce valid incremental JSON from partial invalid JSON based on user provided structures. Effectively this acts as a wrapper for your LLM calls. Due to the nature of this library (it is primarily inteded for use in web servers), it is expected that it will be used in async workflows, and is async first. The library also provides simple HTML templates that serve as examples of how you can integrate the streams in your own components. Due to the nature of partial json streaming, there can be \"wrong\" ways to stream responses that are not effective for partial rendering of responeses in the UI. The library also provides examples of tested ways to apply the library to get good results. High Level Flow Example Component \u00b6 This is an example of a form component being incrementally rendered. By using a structured query response from an LLM, in this case a form with form field names and field placeholders, we can stream the form results directly to a HTML component. This drastically reduces the time to first token, and the precieved time that a user needs to wait. More advanced components are under development. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from stuct_strm import parse_openai from pydantic import BaseModel from openai import AsyncOpenAI ... class DefaultFormItem ( BaseModel ): field_name : str = \"\" field_placeholder : str = \"\" class DefaultFormStruct ( BaseModel ): form_fields : List [ DefaultFormItem ] = [] stream_response = client . beta . chat . completions . stream ( model = \"gpt-4.1\" , messages = messages , response_format = DefaultFormStruct , temperature = 0.0 , ) form_struct_response = parse_openai ( DefaultFormStruct , stream_response ) async for instance in form_struct_response : async for formstruct in instance : print ( formstruct ) Fully formed python classes are returned: 1 2 3 4 >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"\" )]) >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"apple \" )]) >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"apple orange strawberry\" )]) >>> etc.... And the corresponding incomplete json string streams would have looked like: 1 2 3 4 >>> \"{\"form_fields\": [{\"field_name\": \"fruits\" >>> \"{\"form_fields\": [{\"field_name\": \"fruits\", \"field_placeholder\": \"apple \" >>> \"{\"form_fields\": [{\"field_name\": \"fruits\", \"field_placeholder\": \"apple orange strawberry\"} >>> etc... Component Streaming \u00b6 The structured responses can then be easily used to generate incrementally rendered web components. For example this form: Or we can return data in a grid in more interesting ways. For example this rubric: Other \u00b6 I started struct_strm to support another project I'm working on to provide an easy entrypoint for Teachers to use LLM tools in their workflows. Check it out if you're interested - Teachers PET","title":"About"},{"location":"#structured-streamer","text":"struct_strm (structured streamer) is a Python package that makes it easy to stream partial json generated by LLMs into valid json responses. This enables partial rendering of UI components without needing to wait for a full response, drastically reducing the time to the first word on the user's screen.","title":"Structured Streamer"},{"location":"#why-use-structured-streamer","text":"JSON format is the standard when dealing with structured responses from LLMs. In the early days of LLM structured generation we had to validate the JSON response only after the whole JSON response had been returned. Modern approaches use constrained decoding to ensure that only valid json is returned, eliminating the need for post generation validation, and allowing us to use the response imediately. However, the streamed json response is incomplete, so it can't be parsed using traditional methods. This library aims to make it easier to handle this partially generated json to provide a better end user experience. See the benchmarks section in the docs for more details about how this library can speed up your structured response processing. You can learn more about constrained decoding and context free grammar here: XGrammar - Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar","title":"Why Use Structured Streamer?"},{"location":"#installation","text":"1 pip install struct-strm","title":"Installation"},{"location":"#main-features","text":"The primary feature is to wrap LLM outputs to produce valid incremental JSON from partial invalid JSON based on user provided structures. Effectively this acts as a wrapper for your LLM calls. Due to the nature of this library (it is primarily inteded for use in web servers), it is expected that it will be used in async workflows, and is async first. The library also provides simple HTML templates that serve as examples of how you can integrate the streams in your own components. Due to the nature of partial json streaming, there can be \"wrong\" ways to stream responses that are not effective for partial rendering of responeses in the UI. The library also provides examples of tested ways to apply the library to get good results. High Level Flow","title":"Main Features"},{"location":"#example-component","text":"This is an example of a form component being incrementally rendered. By using a structured query response from an LLM, in this case a form with form field names and field placeholders, we can stream the form results directly to a HTML component. This drastically reduces the time to first token, and the precieved time that a user needs to wait. More advanced components are under development. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from stuct_strm import parse_openai from pydantic import BaseModel from openai import AsyncOpenAI ... class DefaultFormItem ( BaseModel ): field_name : str = \"\" field_placeholder : str = \"\" class DefaultFormStruct ( BaseModel ): form_fields : List [ DefaultFormItem ] = [] stream_response = client . beta . chat . completions . stream ( model = \"gpt-4.1\" , messages = messages , response_format = DefaultFormStruct , temperature = 0.0 , ) form_struct_response = parse_openai ( DefaultFormStruct , stream_response ) async for instance in form_struct_response : async for formstruct in instance : print ( formstruct ) Fully formed python classes are returned: 1 2 3 4 >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"\" )]) >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"apple \" )]) >>> DefaultFormStruct ( form_fields =[ DefaultFormItem ( field_name = \"fruits\" , field_placeholder = \"apple orange strawberry\" )]) >>> etc.... And the corresponding incomplete json string streams would have looked like: 1 2 3 4 >>> \"{\"form_fields\": [{\"field_name\": \"fruits\" >>> \"{\"form_fields\": [{\"field_name\": \"fruits\", \"field_placeholder\": \"apple \" >>> \"{\"form_fields\": [{\"field_name\": \"fruits\", \"field_placeholder\": \"apple orange strawberry\"} >>> etc...","title":"Example Component"},{"location":"#component-streaming","text":"The structured responses can then be easily used to generate incrementally rendered web components. For example this form: Or we can return data in a grid in more interesting ways. For example this rubric:","title":"Component Streaming"},{"location":"#other","text":"I started struct_strm to support another project I'm working on to provide an easy entrypoint for Teachers to use LLM tools in their workflows. Check it out if you're interested - Teachers PET","title":"Other"},{"location":"benchmarks/","text":"Benchmarking \u00b6 The main goal of the library is to improve more complex UI facing applications with incremental strucutred responses. The structured responses will start being returned as soon as the first keys of the structs are generated, which will be only slightly after the first tokens are returned. The benchmark is more of a sanity check to confirm this. The benchmark results for OpenAI are shown below. Results \u00b6 From the benchmark resutls we can see that we can return meaningful content to the user around 5x faster than if we waited for the whole fully parseable response. Importantly we can also start showing the user content that they can start reviewing in under a second, just like you can in a chat application. The respones produced a similar number tokens over the testing, which was around 1,400 characters (not tokens). This is roughly around 50 tokens/second, which is inline with normal text generation, but the time to first token time is a little long at a around 500ms. There is a hit compared to unstructured text generation, but that is to be expected due to the grammar model needing to be constructed for the token masking.","title":"Benchmarks"},{"location":"benchmarks/#benchmarking","text":"The main goal of the library is to improve more complex UI facing applications with incremental strucutred responses. The structured responses will start being returned as soon as the first keys of the structs are generated, which will be only slightly after the first tokens are returned. The benchmark is more of a sanity check to confirm this. The benchmark results for OpenAI are shown below.","title":"Benchmarking"},{"location":"benchmarks/#results","text":"From the benchmark resutls we can see that we can return meaningful content to the user around 5x faster than if we waited for the whole fully parseable response. Importantly we can also start showing the user content that they can start reviewing in under a second, just like you can in a chat application. The respones produced a similar number tokens over the testing, which was around 1,400 characters (not tokens). This is roughly around 50 tokens/second, which is inline with normal text generation, but the time to first token time is a little long at a around 500ms. There is a hit compared to unstructured text generation, but that is to be expected due to the grammar model needing to be constructed for the token masking.","title":"Results"},{"location":"examples/","text":"Streaming Examples \u00b6 Any library that supports structured streaming outputs should work with the struct-strm library. However, these use cases aren't widely doumented yet, so I'll add examples here for integrations. OpenAI \u00b6 First we can use the async OpenAI client with a structure to create a stream. Examples of creating the async stream with the OpenAI client can also be found in the OpenAI docs . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from openai import AsyncOpenAI from pydantic import BaseModel client = AsyncOpenAI ( api_key = \"your token here\" ) # Setup up all of the required info for the query class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] # The few shot example is optional few_shot_examples = DefaultListStruct ( items = [ DefaultListItem ( item = \"The Hugging Face Transformers library is an open-source Python library that provides access to a vast collection of pre-trained Transformer models for various machine learning tasks. While initially focused on Natural Language Processing (NLP), its capabilities have expanded to include computer vision, audio processing, and multimodal applications.\" ) ] ) . model_dump_json () query = \"Create list describing 10 open source llm tools\" model = \"gpt-4.1-mini\" # create the messages - messages = [] messages . append ({ \"role\" : \"system\" , \"content\" : f \"example response: { few_shot_examples } \" }) messages . append ({ \"role\" : \"user\" , \"content\" : query }) stream = await client . chat . completions . parse ( model = model , messages = messages , response_format = DefaultListStruct , temperature = 0.0 , ) Now that we have the OpenAI completion stream we can wrap it using the struct-strm library to return structures instead of strings. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from struct_strm import parse_openai_stream import asyncio # you'll want to use a function to handle the async generator async def your_streamed_response_function ( stream , DefaultListStruct ): structured_response_stream = parse_openai_stream ( stream , DefaultListStruct ) async for structure in structured_response_stream : async for list_struct in structure : # do whatever you want with these results print ( list_struct ) # you would probably do something with this function, # but we'll just run it for example purposes asyncio . run ( your_streamed_response_function ( stream , DefaultListStruct )) Fully formed python classes are returned: 1 2 3 4 >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"\" )]) >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"Pytorch\" )]) >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"Pytorch is\" )]) >>> etc.... Hugging Face \u00b6 For an open source approach, we can use Hugging Face (or other libraries) and XGrammar. XGrammar is an open-source library for efficient, flexible, and portable structured generation. XGrammar is an open-source library for efficient, flexible, and portable structured generation. You can read more about XGrammar here , but I'll assume you have already read the docs for this example. There is a bit of setup we need to do, since we will modify the logits of a foundation model to enable the grammar constrained decoding that matches our provided Pydantic (or json) schema. First we'll import everything that we'll need and setup the initial model configs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from threading import Thread from typing import Generator , AsyncGenerator import asyncio import torch from transformers import ( AutoTokenizer , AutoConfig , AutoModelForCausalLM , ) import xgrammar as xgr from transformers import AsyncTextIteratorStreamer from pydantic import BaseModel from struct_strm.llm_wrappers import parse_hf_stream def init_model (): device = 'cpu' # tokenizer info: model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\" model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = torch . float32 , device_map = device ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) config = AutoConfig . from_pretrained ( model_name ) return model , tokenizer , config The bulk of the logic is in this next section. Again, most of this is outlined in the XGrammar docs, but basically we need to: - Create a compiled grammar based on our target structure with our model's tokenizer + XGrammar - Create a logits processor based on the grammar to filter the response tokens to our specified grammar - Start a new thread to support streaming the hugging face model response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 async def test_async_query ( user_query : str , Schema : BaseModel ) -> AsyncGenerator [ str , None ]: model , tokenizer , config = init_model () messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : user_query } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ( text , return_tensors = \"pt\" , ) . to ( model . device ) full_vocab_size = config . vocab_size tokenizer_info = xgr . TokenizerInfo . from_huggingface ( tokenizer , vocab_size = full_vocab_size , ) compiler = xgr . GrammarCompiler ( tokenizer_info ) compiled_grammar = compiler . compile_json_schema ( Schema ) xgr_logits_processor = xgr . contrib . hf . LogitsProcessor ( compiled_grammar ) async_streamer = AsyncTextIteratorStreamer ( tokenizer , timeout = 60.0 , skip_prompt = True , skip_special_tokens = True ) generation_kwargs = dict ( ** model_inputs , max_new_tokens = 20 , logits_processor = [ xgr_logits_processor ], streamer = async_streamer , ) thread = Thread ( target = model . generate , kwargs = generation_kwargs ) thread . start () try : async for token in async_streamer : yield token finally : thread . join () Finally we can take the stream generated by the model + xgrammar and pass it to the struct-strm hugging face wrapper. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async def test_hf_stream ( user_query : str , Schema : BaseModel ) -> AsyncGenerator [ BaseModel , None ]: \"\"\" Test the Hugging Face stream with a given user query and schema. \"\"\" stream = test_async_query ( user_query , Schema ) structured_response_stream = parse_hf_stream ( stream , Schema ) async for structure in structured_response_stream : async for schema_struct in structure : print ( f \"Struct: { type ( schema_struct ) } - { schema_struct } \" ) # run the example class TestPerson ( BaseModel ): name : str = \"\" age : str = \"\" user_query = \"Introduce yourself in JSON with two fields: name and age.\" asyncio . run ( test_hf_stream ( user_query , TestPerson )) Fully formed python classes are returned: 1 2 3 4 >>> TestPerson ( name = \"\" , age = \"\" ) >>> TestPerson ( name = \"bilbo\" , age = \"\" ) >>> TestPerson ( name = \"bilbo baggins\" , age = \"\" ) >>> TestPerson ( name = \"bilbo baggins\" , age = \"111\" )","title":"Examples"},{"location":"examples/#streaming-examples","text":"Any library that supports structured streaming outputs should work with the struct-strm library. However, these use cases aren't widely doumented yet, so I'll add examples here for integrations.","title":"Streaming Examples"},{"location":"examples/#openai","text":"First we can use the async OpenAI client with a structure to create a stream. Examples of creating the async stream with the OpenAI client can also be found in the OpenAI docs . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from openai import AsyncOpenAI from pydantic import BaseModel client = AsyncOpenAI ( api_key = \"your token here\" ) # Setup up all of the required info for the query class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] # The few shot example is optional few_shot_examples = DefaultListStruct ( items = [ DefaultListItem ( item = \"The Hugging Face Transformers library is an open-source Python library that provides access to a vast collection of pre-trained Transformer models for various machine learning tasks. While initially focused on Natural Language Processing (NLP), its capabilities have expanded to include computer vision, audio processing, and multimodal applications.\" ) ] ) . model_dump_json () query = \"Create list describing 10 open source llm tools\" model = \"gpt-4.1-mini\" # create the messages - messages = [] messages . append ({ \"role\" : \"system\" , \"content\" : f \"example response: { few_shot_examples } \" }) messages . append ({ \"role\" : \"user\" , \"content\" : query }) stream = await client . chat . completions . parse ( model = model , messages = messages , response_format = DefaultListStruct , temperature = 0.0 , ) Now that we have the OpenAI completion stream we can wrap it using the struct-strm library to return structures instead of strings. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from struct_strm import parse_openai_stream import asyncio # you'll want to use a function to handle the async generator async def your_streamed_response_function ( stream , DefaultListStruct ): structured_response_stream = parse_openai_stream ( stream , DefaultListStruct ) async for structure in structured_response_stream : async for list_struct in structure : # do whatever you want with these results print ( list_struct ) # you would probably do something with this function, # but we'll just run it for example purposes asyncio . run ( your_streamed_response_function ( stream , DefaultListStruct )) Fully formed python classes are returned: 1 2 3 4 >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"\" )]) >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"Pytorch\" )]) >>> DefaultListStruct ( items =[ DefaultListItem ( item = \"Pytorch is\" )]) >>> etc....","title":"OpenAI"},{"location":"examples/#hugging-face","text":"For an open source approach, we can use Hugging Face (or other libraries) and XGrammar. XGrammar is an open-source library for efficient, flexible, and portable structured generation. XGrammar is an open-source library for efficient, flexible, and portable structured generation. You can read more about XGrammar here , but I'll assume you have already read the docs for this example. There is a bit of setup we need to do, since we will modify the logits of a foundation model to enable the grammar constrained decoding that matches our provided Pydantic (or json) schema. First we'll import everything that we'll need and setup the initial model configs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from threading import Thread from typing import Generator , AsyncGenerator import asyncio import torch from transformers import ( AutoTokenizer , AutoConfig , AutoModelForCausalLM , ) import xgrammar as xgr from transformers import AsyncTextIteratorStreamer from pydantic import BaseModel from struct_strm.llm_wrappers import parse_hf_stream def init_model (): device = 'cpu' # tokenizer info: model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\" model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = torch . float32 , device_map = device ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) config = AutoConfig . from_pretrained ( model_name ) return model , tokenizer , config The bulk of the logic is in this next section. Again, most of this is outlined in the XGrammar docs, but basically we need to: - Create a compiled grammar based on our target structure with our model's tokenizer + XGrammar - Create a logits processor based on the grammar to filter the response tokens to our specified grammar - Start a new thread to support streaming the hugging face model response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 async def test_async_query ( user_query : str , Schema : BaseModel ) -> AsyncGenerator [ str , None ]: model , tokenizer , config = init_model () messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : user_query } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ( text , return_tensors = \"pt\" , ) . to ( model . device ) full_vocab_size = config . vocab_size tokenizer_info = xgr . TokenizerInfo . from_huggingface ( tokenizer , vocab_size = full_vocab_size , ) compiler = xgr . GrammarCompiler ( tokenizer_info ) compiled_grammar = compiler . compile_json_schema ( Schema ) xgr_logits_processor = xgr . contrib . hf . LogitsProcessor ( compiled_grammar ) async_streamer = AsyncTextIteratorStreamer ( tokenizer , timeout = 60.0 , skip_prompt = True , skip_special_tokens = True ) generation_kwargs = dict ( ** model_inputs , max_new_tokens = 20 , logits_processor = [ xgr_logits_processor ], streamer = async_streamer , ) thread = Thread ( target = model . generate , kwargs = generation_kwargs ) thread . start () try : async for token in async_streamer : yield token finally : thread . join () Finally we can take the stream generated by the model + xgrammar and pass it to the struct-strm hugging face wrapper. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 async def test_hf_stream ( user_query : str , Schema : BaseModel ) -> AsyncGenerator [ BaseModel , None ]: \"\"\" Test the Hugging Face stream with a given user query and schema. \"\"\" stream = test_async_query ( user_query , Schema ) structured_response_stream = parse_hf_stream ( stream , Schema ) async for structure in structured_response_stream : async for schema_struct in structure : print ( f \"Struct: { type ( schema_struct ) } - { schema_struct } \" ) # run the example class TestPerson ( BaseModel ): name : str = \"\" age : str = \"\" user_query = \"Introduce yourself in JSON with two fields: name and age.\" asyncio . run ( test_hf_stream ( user_query , TestPerson )) Fully formed python classes are returned: 1 2 3 4 >>> TestPerson ( name = \"\" , age = \"\" ) >>> TestPerson ( name = \"bilbo\" , age = \"\" ) >>> TestPerson ( name = \"bilbo baggins\" , age = \"\" ) >>> TestPerson ( name = \"bilbo baggins\" , age = \"111\" )","title":"Hugging Face"},{"location":"getting_started/","text":"Installation \u00b6 Install With Pypi: 1 pip install struct-strm Usage \u00b6 This is an example of a simple case where we return a list from a structured OpenAI streamed llm output. Despite json strings being returned, struct_strm is able to return an incrementally populated list object as the response. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import asyncio from struct_strm import parse_openai_stream # The query and context for the LLM call prompt_context = \"\" user_query = \"Create list describing 5 open source llm tools\" class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] TestStruct = DefaultListStruct few_shot_examples = DefaultListStruct ( items = [ DefaultListItem ( item = \"The Hugging Face Transformers library is an open-source Python library that provides access to a vast collection of pre-trained Transformer models for various machine learning tasks. While initially focused on Natural Language Processing (NLP), its capabilities have expanded to include computer vision, audio processing, and multimodal applications.\" ) ] ) . model_dump_json () prompt_context = \"\" query = \"Create list describing 10 open source llm tools\" model = \"gpt-4.1-mini\" client = await aget_openai_client () messages = [] messages . append ({ \"role\" : \"system\" , \"content\" : prompt_context }) if few_shot_examples is not None : messages . append ({ \"role\" : \"system\" , \"content\" : f \"example response: { few_shot_examples } \" }) messages . append ({ \"role\" : \"user\" , \"content\" : query }) stream = client . beta . chat . completions . stream ( model = model , messages = messages , response_format = TestStruct , temperature = 0.0 , ) async def test_list_parse ( stream ): structured_response_stream = parse_openai_stream ( stream , TestStruct ) async for structure in structured_response_stream : async for list_struct in structure : print ( list_struct ) asyncio . run ( test_list_parse ( stream )) As we loop through the items in the async for loop: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugg\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Trans\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers:\")]) >>> etc... >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp:\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp: A\")]) >>> etc... HTML Components \u00b6 This library also provides some convience functions for serving html components (htmx style and vanilla javascript) direclty to the frontend. Reference implementations can be found in the tests/02_app directory. If you use frontend frameworks like React, you can ignore this section and directly use the core library. Since we are streaming structured outputs for the purpose of using them in the UI, we know we need several features, such as a pre-response placeholder, streaming reading/updates, and a final post response indicator. Right now I have some minimal implementations included in this library. They are not super customizable yet, but can serve as references for your projects. Example usage with FastAPI and HTMX: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from struct_strm.structs.list_structs import DefaultListStruct from struct_strm.ui_components import ListComponent from struct_strm.structs.list_structs import simulate_stream_list_struct from fastapi import FastAPI , Request from fastapi.responses import HTMLResponse from sse_starlette.sse import EventSourceResponse from fastapi.templating import Jinja2Templates app = FastAPI () templates = Jinja2Templates ( directory = \"tests/app\" ) @app . get ( \"/get_list_stream\" ) def test_fetch_list_sse (): # kick off SSE stream sse_container = \"sse-list\" stream_target = \"stream-list\" component_path = \"/test_list\" sse_html = f \"\"\"<div id=\"sse-list-container\" hx-ext=\"sse\" sse-connect=\" { component_path } \"> <div sse-swap=\"message\" hx-target=\"# { stream_target } \" hx-swap=\"innerHTML\"> </div> <div sse-swap=\"streamCompleted\" hx-target=\"# { sse_container } \"> </div> </div> \"\"\" return HTMLResponse ( content = sse_html , media_type = \"text/html\" ) @app . get ( \"/test_list\" ) async def test_list (): component = ListComponent () stream : AsyncGenerator = simulate_stream_list_struct ( interval_sec = 0.02 ) html_component_stream : AsyncGenerator = component . render ( response_stream = stream ) async def wrapper (): async for item in html_component_stream : print ( item ) yield item yield { \"event\" : \"streamCompleted\" , \"data\" : \"\" } return EventSourceResponse ( wrapper (), media_type = \"text/event-stream\" )","title":"Getting Started"},{"location":"getting_started/#installation","text":"Install With Pypi: 1 pip install struct-strm","title":"Installation"},{"location":"getting_started/#usage","text":"This is an example of a simple case where we return a list from a structured OpenAI streamed llm output. Despite json strings being returned, struct_strm is able to return an incrementally populated list object as the response. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import asyncio from struct_strm import parse_openai_stream # The query and context for the LLM call prompt_context = \"\" user_query = \"Create list describing 5 open source llm tools\" class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] TestStruct = DefaultListStruct few_shot_examples = DefaultListStruct ( items = [ DefaultListItem ( item = \"The Hugging Face Transformers library is an open-source Python library that provides access to a vast collection of pre-trained Transformer models for various machine learning tasks. While initially focused on Natural Language Processing (NLP), its capabilities have expanded to include computer vision, audio processing, and multimodal applications.\" ) ] ) . model_dump_json () prompt_context = \"\" query = \"Create list describing 10 open source llm tools\" model = \"gpt-4.1-mini\" client = await aget_openai_client () messages = [] messages . append ({ \"role\" : \"system\" , \"content\" : prompt_context }) if few_shot_examples is not None : messages . append ({ \"role\" : \"system\" , \"content\" : f \"example response: { few_shot_examples } \" }) messages . append ({ \"role\" : \"user\" , \"content\" : query }) stream = client . beta . chat . completions . stream ( model = model , messages = messages , response_format = TestStruct , temperature = 0.0 , ) async def test_list_parse ( stream ): structured_response_stream = parse_openai_stream ( stream , TestStruct ) async for structure in structured_response_stream : async for list_struct in structure : print ( list_struct ) asyncio . run ( test_list_parse ( stream )) As we loop through the items in the async for loop: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugg\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Trans\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers:\")]) >>> etc... >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp:\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"Hugging Face Transformers: A popular open-source library etc....\"), DefaultListItem(item=\"Llama.cpp: A\")]) >>> etc...","title":"Usage"},{"location":"getting_started/#html-components","text":"This library also provides some convience functions for serving html components (htmx style and vanilla javascript) direclty to the frontend. Reference implementations can be found in the tests/02_app directory. If you use frontend frameworks like React, you can ignore this section and directly use the core library. Since we are streaming structured outputs for the purpose of using them in the UI, we know we need several features, such as a pre-response placeholder, streaming reading/updates, and a final post response indicator. Right now I have some minimal implementations included in this library. They are not super customizable yet, but can serve as references for your projects. Example usage with FastAPI and HTMX: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from struct_strm.structs.list_structs import DefaultListStruct from struct_strm.ui_components import ListComponent from struct_strm.structs.list_structs import simulate_stream_list_struct from fastapi import FastAPI , Request from fastapi.responses import HTMLResponse from sse_starlette.sse import EventSourceResponse from fastapi.templating import Jinja2Templates app = FastAPI () templates = Jinja2Templates ( directory = \"tests/app\" ) @app . get ( \"/get_list_stream\" ) def test_fetch_list_sse (): # kick off SSE stream sse_container = \"sse-list\" stream_target = \"stream-list\" component_path = \"/test_list\" sse_html = f \"\"\"<div id=\"sse-list-container\" hx-ext=\"sse\" sse-connect=\" { component_path } \"> <div sse-swap=\"message\" hx-target=\"# { stream_target } \" hx-swap=\"innerHTML\"> </div> <div sse-swap=\"streamCompleted\" hx-target=\"# { sse_container } \"> </div> </div> \"\"\" return HTMLResponse ( content = sse_html , media_type = \"text/html\" ) @app . get ( \"/test_list\" ) async def test_list (): component = ListComponent () stream : AsyncGenerator = simulate_stream_list_struct ( interval_sec = 0.02 ) html_component_stream : AsyncGenerator = component . render ( response_stream = stream ) async def wrapper (): async for item in html_component_stream : print ( item ) yield item yield { \"event\" : \"streamCompleted\" , \"data\" : \"\" } return EventSourceResponse ( wrapper (), media_type = \"text/event-stream\" )","title":"HTML Components"},{"location":"limitations/","text":"Limitations \u00b6 There are still many limiations for the parsing that I'm working to address. The current setup works for my use cases, but is far from ideal. Some limitations include: 1. Only flat structs, or structs with up to one nextested level are supported \u00b6 Suported: 1 2 3 4 5 6 7 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] Not supported yet: 1 2 3 4 5 6 7 8 9 class DefaultListItemDetail ( BaseModel ): detail : str = \"\" class DefaultListItem ( BaseModel ): item : DefaultListItemDetail = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] 2. I'm not handling any data types beyond strings and arrays (no ints/floats or optional types) \u00b6 Suported: 1 2 class DefaultListItem ( BaseModel ): item : str = \"\" Not supported yet: 1 2 class DefaultListItem ( BaseModel ): item : int = \"\" 3. Nested structures must have different value names \u00b6 Suported: 1 2 3 4 5 6 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] Not supported yet: 1 2 3 4 5 6 class DefaultListItem ( BaseModel ): items : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] 4. I'm in the process of adding support for dataclasses \u00b6 Currenlty pydantic models are the only structures supported","title":"Limitations"},{"location":"limitations/#limitations","text":"There are still many limiations for the parsing that I'm working to address. The current setup works for my use cases, but is far from ideal. Some limitations include:","title":"Limitations"},{"location":"limitations/#1-only-flat-structs-or-structs-with-up-to-one-nextested-level-are-supported","text":"Suported: 1 2 3 4 5 6 7 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] Not supported yet: 1 2 3 4 5 6 7 8 9 class DefaultListItemDetail ( BaseModel ): detail : str = \"\" class DefaultListItem ( BaseModel ): item : DefaultListItemDetail = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = []","title":"1. Only flat structs, or structs with up to one nextested level are supported"},{"location":"limitations/#2-im-not-handling-any-data-types-beyond-strings-and-arrays-no-intsfloats-or-optional-types","text":"Suported: 1 2 class DefaultListItem ( BaseModel ): item : str = \"\" Not supported yet: 1 2 class DefaultListItem ( BaseModel ): item : int = \"\"","title":"2. I'm not handling any data types beyond strings and arrays (no ints/floats or optional types)"},{"location":"limitations/#3-nested-structures-must-have-different-value-names","text":"Suported: 1 2 3 4 5 6 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] Not supported yet: 1 2 3 4 5 6 class DefaultListItem ( BaseModel ): items : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = []","title":"3. Nested structures must have different value names"},{"location":"limitations/#4-im-in-the-process-of-adding-support-for-dataclasses","text":"Currenlty pydantic models are the only structures supported","title":"4. I'm in the process of adding support for dataclasses"},{"location":"parsing_streams/","text":"Technical Implementation \u00b6 The library relies heavliy on the tree sitter and tree sitter python bindings for efficient parsing. I tried a couple simpler parsing approaches at first, and it quickly realized that it is a complex problem, and tree sitter has already been built by much smarter people to handle these complexities. Since we are working with structured text streams, our use cases overlap nicely with the text editor use case that tree sitter is primarily built for. Tree Sitter Docs: https://tree-sitter.github.io/tree-sitter/ Parsing \u00b6 The goal of the parsing is to always output fully constructed python structures. This means that all python structures must have default values that will be returned when there are no matching nodes in the parse tree. You can checkout the /src/struct_strm/structs/ folder for examples of what kind of structures are expected, and examples of streams for those components. In the trivial case of a structured list, the structures may look like: 1 2 3 4 5 6 7 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] The structures can then be used both for the OpenAI request and the parsing method from this library. Since the whole point of this library is to stream responses, async + generators are the supported response types. For each itteration a new instance of the passed structure ( DefaultListStruct in this case) is returned. Not the most efficient, but hey this is Python. The heavy duty processing is handled efficeintly by tree sitter. 1 2 3 4 list_items_response : AsyncGenerator [ DefaultListStruct , None ] = tree_sitter_parse ( DefaultListStruct , response_stream , ) The response for this parsed example would look something like: 1 2 3 4 5 6 >>> DefaultListStruct(items=[]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple\"),]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"),]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"), DefaultListItem(item=\"banana\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"), DefaultListItem(item=\"banana kiwi grape\")]) >>> etc... UI Integration \u00b6 Most of my examples are tested with HTMX, since it is easy to setup in a simple python project for testing. I imagine most users will dump the results to json format for use with front ends like React.","title":"Features"},{"location":"parsing_streams/#technical-implementation","text":"The library relies heavliy on the tree sitter and tree sitter python bindings for efficient parsing. I tried a couple simpler parsing approaches at first, and it quickly realized that it is a complex problem, and tree sitter has already been built by much smarter people to handle these complexities. Since we are working with structured text streams, our use cases overlap nicely with the text editor use case that tree sitter is primarily built for. Tree Sitter Docs: https://tree-sitter.github.io/tree-sitter/","title":"Technical Implementation"},{"location":"parsing_streams/#parsing","text":"The goal of the parsing is to always output fully constructed python structures. This means that all python structures must have default values that will be returned when there are no matching nodes in the parse tree. You can checkout the /src/struct_strm/structs/ folder for examples of what kind of structures are expected, and examples of streams for those components. In the trivial case of a structured list, the structures may look like: 1 2 3 4 5 6 7 class DefaultListItem ( BaseModel ): item : str = \"\" class DefaultListStruct ( BaseModel ): # mostly just for testing items : list [ DefaultListItem ] = [] The structures can then be used both for the OpenAI request and the parsing method from this library. Since the whole point of this library is to stream responses, async + generators are the supported response types. For each itteration a new instance of the passed structure ( DefaultListStruct in this case) is returned. Not the most efficient, but hey this is Python. The heavy duty processing is handled efficeintly by tree sitter. 1 2 3 4 list_items_response : AsyncGenerator [ DefaultListStruct , None ] = tree_sitter_parse ( DefaultListStruct , response_stream , ) The response for this parsed example would look something like: 1 2 3 4 5 6 >>> DefaultListStruct(items=[]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple\"),]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"),]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"), DefaultListItem(item=\"banana\")]) >>> DefaultListStruct(items=[DefaultListItem(item=\"apple orange strawberry\"), DefaultListItem(item=\"banana kiwi grape\")]) >>> etc...","title":"Parsing"},{"location":"parsing_streams/#ui-integration","text":"Most of my examples are tested with HTMX, since it is easy to setup in a simple python project for testing. I imagine most users will dump the results to json format for use with front ends like React.","title":"UI Integration"}]}